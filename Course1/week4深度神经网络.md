</br>
    
# 深度L层神经网络

</br>

![e68b5be7-3ab4-472d-8223-20f310894e5a](https://github.com/user-attachments/assets/f7961837-8f56-4025-8027-67175c1f6c93)

<p>logistic regression</p>

![d1401aa8-6a63-4722-93fe-593d8c9aa09a](https://github.com/user-attachments/assets/b4759ab7-be65-4399-9d34-e61d64e2c946)

<p>1 hidden layer</p>

<p>来看看更深的</p>

![e2f9d210-ce19-4359-bb16-f8f0aa3e42ab](https://github.com/user-attachments/assets/3708979c-8723-4fad-a8f8-949a313d3b91)

![47fc540a-5d55-4734-b5dd-949d8015fa12](https://github.com/user-attachments/assets/678ca35e-dcff-4996-9a9a-855baefae2e3)

<p>对于任何特定的问题来说，可能很难事先得知你需要多深的网络，所以一般会先尝试逻辑回归，然后再尝试一个隐藏层，两个隐藏层。<b>可以把隐藏层的数量作为另一个超参数</b>，可以尝试很多不同的值，然后通过交叉验证或者开发集进行评估</p>

<p>来到喜闻乐见的专业术语：</p>

- L: 神经网络的层数

- n^[l] = 第 l 层上的单元数（n^[0] = n_x）

- 对于每一层 l，使用 a[l] 代表第 l 层中的激活函数（a[l] = g[l](z[l])）

- W[l] 表示 l 层计算中间值 z[l] 的权重

- a[L] = yhat

</br>

# 深度网络中的前向传播

</br>

![8a1d8ef2-0524-452d-a712-ff15734ea6ad](https://github.com/user-attachments/assets/cb0449a2-7bc3-40a3-8ebd-300010fb7363)

<p>和往常一样，先来看对于单独训练样本 x，如何实现前向传播</p>

<p>先计算</p>

![QianJianTec1740317014171](https://github.com/user-attachments/assets/01fe7b70-26a0-4b0e-a246-372bd1da77ae)

<p>x 这里也是 a[0]，然后计算着一层的激活函数</p>

![QianJianTec1740317261438](https://github.com/user-attachments/assets/44eb304d-053b-4898-8187-bb75b70aa1e1)

<p>得出第一个隐藏层的计算公式</p>

<p>关于第二个隐藏层</p>

![QianJianTec1740317602445](https://github.com/user-attachments/assets/3e80edaf-72bd-4c78-b035-0c2ff2ac9500)

![QianJianTec1740317684806](https://github.com/user-attachments/assets/f0eb63ee-d4ad-4798-87f6-33b114b879f7)

<p>后面几层以此类推直到输出层。完成对单一训练样本的前向传播通用公式</p>

<p>向量化只需把 x 换成 X，z 换成 Z，a 换成 A。（把他们堆叠起来变成矩阵）</p>

<p>这里也有一个循环：for l = 1...4。之前说过在使用神经网络时，要尽可能避免使用 for-loop。这里是唯一一处，我觉得除了for以外没有更好解决的地方</p>

</br>

# 正确处理矩阵尺寸

</br>

<p>W[l] : (n[l], n[l - 1])</p>

<p>b[l] : (n[l], 1)</p>

<p>如果正在实现反向传播，那么 dW 的维度与 W 相同</p>

<p>l 层的 a 与 z 维度相同为 (n[l], m)</p>

<p>上一层的神经元数量表示权重矩阵的列数，而当前层的神经元数量表示权重矩阵的行数。</p>

</br>

# 为什么需要深度表征

</br>

<p>深度神经网络对于很多问题确实很有效，具体而言，他们需要是深度的(有很多隐藏层)</p>

<p>给出一个例子，来看看为什么深度网络有用：</p>

<p>搭建一个面部识别系统，那么神经网络就可以在此运用，输入一张面部图片。那么，神经网络的第一层可以被认为是一个特征检测器或者边缘检测器。</p>

![cc6cebef-463a-4665-8724-ed50e9895f7f](https://github.com/user-attachments/assets/183743a6-1756-4d80-88c6-d1b47d59c5ed)

<p>搭建一个具有二十个隐藏神经元的神经网络，可能是图像上的某种算法，这二十个隐藏神经元通过这些小方块可视化。例如(4, 1)这个微型可视化图表示一个隐藏神经元正在试图找出在 DMH 中该方向的边缘位置，另一个隐藏神经元可能试图找出这幅图像中的水平边缘在哪里，在后面的卷积网络中，这个特殊的可视化可能会更有意义</p>

<p>但是形式上，可以认为神经网络的第一层就好比看一张图片，通过将像素分组来形成边缘的方法，找出这张图片的边缘，然后可以取消检测边缘并将边缘组合在一起，以形成面部的一部分</p>

<p>例如，可能有一个神经元试图发现一个眼睛，或者一个不同的神经元试图找到鼻子的一部分。所以通过把大量的边缘放置在一起，可以开始检测面部的不同部位，最后通过将面部的不同部位(eye nose)组合在一起。然后可以尝试识别或检测不同类型的面部</p>

![2fb52011-fed3-4dd5-8bde-ee83333d1f87](https://github.com/user-attachments/assets/cd0093c1-bc32-4248-8ba7-91b2193662e2)

<p>所以直观地，可以<b>将神经网络的浅层看作是简单的检测函数，如检测边缘。然后在神经网络的后一层，将它们组合在一起，以便他可以学习更多和更复杂的功能</b>。当我们讨论卷积网络时，这些可视化将更有意义</p>

<p>值得注意的是，这种可视化的一个技术细节：<b>边缘检测器检测图像中相对较小的区域，然后面部检测器可能会看到更大的图像区域</b>。但是从中获取的进一步的关键信息，仅仅是找到像边缘这样的简单事物，然后构建他们，将它们组合在一起以检测更复杂的事物，然后再次将它们组合在一起以找到更复杂的事物</p>

<p><b>这种由简单到复杂的分层表示或组合表示不仅适用于图像和面部识别也适用于其他类型的数据</b>，例如建立语音识别系统，很难将 speech 可视化，但如果输入一段音频剪辑那么神经网络的第一层可能学习检测低级的音频波形特征（比如音调是否升高）。当涉及到这一点时和上面一样，检测低级的波形特征，然后通过组合低级波形特征，就可以学到检测基本的声音单位（音素）。例如，在 cat 这个词中，C 是一个音素，A 是一个音素，T 又是另一个音素，学习寻找可能是语音基本单元的声音，然后把他们组合在一起，可以用来识别语音中的单词，然后再把他们组合在一起，用以识别整个短语或句子</p>

<p>因此，拥有众多隐藏层的深度神经网络可能能够让前面的的神经层学习这些较低级别的简单特征，然后让后面更深的神经层汇聚前面所检测到的简单信息，以便检测更复杂的事物</p>

<p>正如所见，尽管其他神经层正在计算着类似边缘这类似乎相对简单的输入函数，而当深入研究神经网络时，实际上可以做一些令人惊讶复杂的事情（检测面部，检测单词短语句子）</p>

<p>或许大脑也一样，开始也检测简单的东西如你所看到的边缘，然后将这些信息建立起来，以检测更复杂的东西</p>

<p><b>有点凝聚态物理的感觉</b></p>

<p>关于深度神经网络有另一个 work well 的 intuition（直觉）：有一些函数可以用一个‘小的’L层深度神经网络来计算，而浅层网络则需要指数级更多的隐藏单元来计算。</p>

<p>这意味着，在一些情况下，深层神经网络（即具有更多层的网络）能够以较少的计算量来表示和计算某些函数，而浅层网络（即层数较少的网络）则可能需要非常多的计算单元（隐藏单元）才能达到相同的效果。这个结论来源于电路理论，表明通过增加网络的深度（即更多的层），可以在计算上实现更高效的表示。</p>

<p>电路理论涉及到什么类型的函数可以用不同的（与 或 非）逻辑情况来计算。</p>

<p>所以非正式的说法，我们可以用相对较小（隐藏神经元数量相对较小）但具有深度的神经网络来计算这些功能。但是如果尝试用浅网络计算相同的功能，可能需要指数级的神经元（unit）来进行计算。参考电路理论的异或树，只需要对数级的门，如果强制的只使用一层，则门数量会指数级的扩大（因为要穷举 2^n 种可能性）</p>

<p>对于一些问题，使用深层网络进行计算比使用浅层网络更容易</p>

<p>Andrew Ng：大家都形成了一个概念那就是深度网络很有效，有时人们滥用神经网络，弄太多隐藏层了，但是当我开始解决一个新问题时首先采用的甚至是逻辑回归，然后尝试一个或两个的隐藏层，<b>把它当作超参数，将其作用于我们调整的参数或超参数来为我们的神经网络找到合适的深度</b></p>

</br>

# 深度神经网络的架构

</br>

<p>之前已经了解了前向传播和后向传播的基本框架</p>

![d274f4e4-a4b3-418e-b21a-86244c3c1436](https://github.com/user-attachments/assets/ac0e20be-8029-4696-8350-95851edee857)

<p>只选一层，并只关注该层中的计算</p>

<p>对于前向传播，关于 Layer l 这就是如何从输入计算到输出</p>

![QianJianTec1740576818874](https://github.com/user-attachments/assets/d939d313-58be-4057-817c-db8a195166b1)

![QianJianTec1740576880455](https://github.com/user-attachments/assets/fdbaf156-d766-43f2-b376-1104431ca05c)

<p>在之后的计算中会发现，将 Z[l] 的值缓存是非常有用的，因为对反向传播很有用</p>

<p>对于反向传播，我们只关注 l 层的计算。将实现一个函数，输入为 da[l] 和缓存（前面已经计算好的 Z[l]），输出为 da[l - 1]</p>

![IMG_0208](https://github.com/user-attachments/assets/6d3b7fd5-619e-4b3b-ab2f-e84a00b03834)

<p>这就是前向传播和反向传播的框架</p>

<p>实现深度神经网络的基本框架：在每一层中，有前向传播，反向传播，还有他们之间传递值的缓存</p>

![46869bcc-fb72-46c4-8b1d-4742b347fa86](https://github.com/user-attachments/assets/c91f97d9-6382-4080-804c-49599064bb50)

<p>得到了所有导数项，然后去更新参数</p>

</br>

# 前向传播和后向传播

</br>

<p>讨论如何实现前面的架构</p>

<p>从正向传播开始</p>

```
Input a[l - 1]
Output a[l], cache(z[l])
从实现的角度看，为了让函数的调用在程序练习中更容易一些，我们还会缓存 w[l] 和 b[l]
```

<p>Vectorized:</p>

```
Z[l] = W[l] A[l - 1] + b[l]
A[l] = g[l](Z[l])
```

<p>根据上面的图，我们有一系列的盒子，可以用 A[0](x) 进行初始化实现从左到右的正向传播</p>

<p>讨论反向传播</p>

```
Input da[l]
Output da[l - 1], dW[l], db[l]
```

```
dz[l] = da[l] * g[l]'(z[l])
dW[l] = dz[l] · a[l - 1]^T
db[l] = dz[l]
da[l - 1] = W[l]^T · dz[l]
```

```
Vectorized:
dZ[l] = dA[l] * g[l]'(Z[l])
dW[l] = dZ[l] · A[l - 1]^T / m
db[l] = np.sum(dZ[l], axis = 1, keepdims = true) / m
dA[l - 1] = W[l]^T · dZ[l]
```

![1787cdc0-634a-4914-88a7-7f222482eccd](https://github.com/user-attachments/assets/3460445c-ed96-47f4-a212-4193974ace48)

<p>用这四个方程来实现你的反向函数</p>

`dz[l] = W[l+1]^T dz[l+1] * g[l]'(z[l])`

<p>这是之前实现的只有一个隐藏层的神经网络用到的公式</p>

<p>Summarize：以输入 x 为例，可能第一层有 ReLU 激活功能。然后转到第二层也许使用另一个ReLU激活函数。进入第三层，如果你正在进行二分类，则有可能有 sigmoid 激活函数，这会输出 yhat。然后用 yhat，可以计算损失。这使得你可以开始向后迭代，将让反向传播计算导数：dW[3], db[3], dW[2], db[2], dW[1], db[1]。一路走来，会用 cache 进行计算，所以我们将 z[1], z[2], z[3]。在反向传播单元中，可以向后传递 da[2] 和 da[1]</p>

<p>这就是为三层神经网络实现前向传播和后向传播的方式</p>

![46e842a0-15a9-4c84-a03c-1315f7c4ee23](https://github.com/user-attachments/assets/97b8ff4d-9559-430d-b4e4-365d7348da4f)

![0cb465c1-112c-4d0b-99ea-06711215b447](https://github.com/user-attachments/assets/c8d58068-c5ee-4df9-b817-ad650653d385)

<p>你的学习算法的复杂性大多来自于数据而不是你的代码</p>

</br>

# 参数与超参数

</br>

<p>要有效地开发深度神经网络，不仅需要很好的组织你的参数，还有超参数</p>

<p>Hyperparameters：</p>

- learning rate alpha：决定参数如何演变

- iterations

- hidden layer L

- hidden units：n[1], n[2], ...

- Choice of activation functions

<p>超参数以某种方式决定参数 W 和 b 最终的值。后面还有更多的超参数</p>

<p>当你训练深度神经网络时，会发现有很多可能的设置，要做的只是尝试</p>

<p>比如对 alpha</p>

![696c2bc9-0773-4012-90e2-db79a929452b](https://github.com/user-attachments/assets/1def0f3e-0314-4d51-b078-8daedc5c790a)

<p>对于某一个 alpha 使得深网能够快速的学习，并且允许收敛</p>

<p>刚开始一个神经网络时，很难事先知道超参数的最佳值是多少</p>






































































































