# Gradient Descent

</br>

```
def update_parameters_with_gd(parameters, grads, learning_rate):
    L = len(parameters) // 2 # number of layers in the neural networks

    # Update rule for each parameter
    for i in range(L):
        ### START CODE HERE ### (approx. 2 lines)
        parameters["W" + str(i + 1)] -= learning_rate * grads["dW" + str(i + 1)]
        parameters["b" + str(i + 1)] -= learning_rate * grads["db" + str(i + 1)]
        ### END CODE HERE ###
        
    return parameters
```

<p>随机梯度下降 (SGD) 是批量梯度下降的变体，它相当于小批量梯度下降，但每个小批量只有 1 个样本。实现的更新规则保持不变。不同的是，您每次只针对一个训练样本计算梯度，而不是针对整个训练集。</p>

```
X = data_input
Y = labels
parameters = initialize_parameters(layers_dims)
for i in range(0, num_iterations):
    for j in range(0, m):
        # Forward propagation
        a, caches = forward_propagation(X[:,j], parameters)
        # Compute cost
        cost = compute_cost(a, Y[:,j])
        # Backward propagation
        grads = backward_propagation(a, caches, parameters)
        # Update parameters.
        parameters = update_parameters(parameters, grads)
```

<p>在随机梯度下降法中，在更新梯度之前仅使用 1 个训练样本。当训练集较大时，SGD 的速度可能会更快。但参数会“震荡”向最小值，而不是平滑收敛。</p>

![QQ_1750235035569](https://github.com/user-attachments/assets/c9494966-c174-4786-bfad-0ab38e79d67e)

<p>SGD 会导致多次振荡才能达到收敛。但 SGD 的每个步骤的计算速度比 GD 快得多，因为它只使用一个训练样本（而 GD 则使用整个批次）。</p>

</br>

# Mini-Batch Gradient descent

</br>

<p>通常其的表现优于梯度下降或随机梯度下降（特别是当训练集很大时）。</p>






































































































































































































































































































































































































































































































































































































































































































































