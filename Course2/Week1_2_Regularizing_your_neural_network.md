# Regularization

</br>

<p>如果你怀疑你的神经网络在数据上发生了过拟合（存在高方差问题），首先尝试使用正则化，获取更多数据也是一个可靠的方法，但不是总能获取数据</p>

</br>

## 逻辑回归

</br>

<p>以逻辑回归为例子进行阐述，在逻辑回归中，会尝试最小化代价函数 J：</p>

$$
J(w, b) = \frac{1}{m} \sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) 
$$

<p>w 是 n_x 维的参数向量，b 是一个实数</p>

<p>要为逻辑回归正则化，需要加上 lambda，他称为正则化参数</p>

$$
J(w, b) = \frac{1}{m} \sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}{\|w\|_2}^2
$$

<p>其中：</p>

$$
{\|w\|_2}^2 = \sum_{j = 1}^{n_x}{w_j}^2 = w^T w 
$$

<p><b>这称为 L2 正则化</b>，其为 L2 范数</p>

<p>关于为何只对 w 进行正则化：事实上也可以加上 b 的范数，但通常会把他省略掉。</p>

<p>观察参数，w 往往是一个非常高维的参数矢量，尤其是发生在高方差问题的情况下，w 可能有非常多的参数。没能很好的拟合所有参数，而 b 只是单个数字，几乎所有参数都集中在 w 中，而不是 b。所以即使加上最后一项，也不会起到很大的作用（因为 b 只是大量参数中的一个）</p>

<p>L2 正则化是最常见的正则化方式</p>

<p>可能也听过 L1 正则化，即不使用 L2 范数，而是使用：</p>

$$
\frac{\lambda}{m}\sum_{j = 1}^{n_x}|w| = \frac{\lambda}{m}\|w\|_1
$$

<p>称为参数矢量 w 的 L1 范数（无论是 m 还是 2m 都只是一个缩放变量）</p>

<p>如果使用 L1 正则化，w 最后会变得稀疏，意味着 w 矢量中有很多 0。有人认为这有助于压缩模型，因为有一部分参数是 0，只需较少的内存来存储模型</p>

<p>在实践中发现，通过 L1 正则化让模型变得稀疏，带来的收益甚微，所以觉得至少在压缩模型的目标上他作用不大。</p>

<p>在训练神经网络时，L2 正则化使用得频繁很多</p>

<hr>

<p>lambda 被称为正则化参数，通常使用开发集或 hold-out 交叉验证，来配置参数。通过尝试一系列的值，找出最好的那个，即在“训练集上得到较好的结果”和“保持参数 w 的 L2 范数较小以避免过拟合”之间取舍</p>

<p>lambda 是你需要调优的另一个超参数</p>

<p>以上是在逻辑回归中实现 L2 正则化的方法</p>

</br>

## 神经网络

</br>

<p>在神经网络中，代价函数是所有参数的函数，w[1] b[1] 到 w[L] b[L]</p>

<p>至于正则化，再加上一项</p>

$$
J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l = 1}^{L}{\|w^{[l]}\|_F}^2
$$

$$
{\|w^{[l]}\|}^2 = \sum_{i = 1}^{n^{[l]}}\sum_{j = 1}^{n^{[l - 1]}}(w_{ij}^{[l]})^2
$$

<p>称为矩阵的弗罗贝尼乌斯范数</p>

</br>

## 实现梯度下降

</br>








































































































































































































































































































































































































































































































































































































