# Regularization

</br>

<p>如果你怀疑你的神经网络在数据上发生了过拟合（存在高方差问题），首先尝试使用正则化，获取更多数据也是一个可靠的方法，但不是总能获取数据</p>

</br>

## 逻辑回归

</br>

<p>以逻辑回归为例子进行阐述，在逻辑回归中，会尝试最小化代价函数 J：</p>

$$
J(w, b) = \frac{1}{m} \sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) 
$$

<p>w 是 n_x 维的参数向量，b 是一个实数</p>

<p>要为逻辑回归正则化，需要加上 lambda，他称为正则化参数</p>

$$
J(w, b) = \frac{1}{m} \sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}{\|w\|_2}^2
$$

<p>其中：</p>

$$
{\|w\|_2}^2 = \sum_{j = 1}^{n_x}{w_j}^2 = w^T w 
$$

<p><b>这称为 L2 正则化</b>，其为 L2 范数</p>

<p>关于为何只对 w 进行正则化：事实上也可以加上 b 的范数，但通常会把他省略掉。</p>

<p>观察参数，w 往往是一个非常高维的参数矢量，尤其是发生在高方差问题的情况下，w 可能有非常多的参数。没能很好的拟合所有参数，而 b 只是单个数字，几乎所有参数都集中在 w 中，而不是 b。所以即使加上最后一项，也不会起到很大的作用（因为 b 只是大量参数中的一个）</p>

<p>L2 正则化是最常见的正则化方式</p>

<p>可能也听过 L1 正则化，即不使用 L2 范数，而是使用：</p>

$$
\frac{\lambda}{m}\sum_{j = 1}^{n_x}|w| = \frac{\lambda}{m}\|w\|_1
$$

<p>称为参数矢量 w 的 L1 范数（无论是 m 还是 2m 都只是一个缩放变量）</p>

<p>如果使用 L1 正则化，w 最后会变得稀疏，意味着 w 矢量中有很多 0。有人认为这有助于压缩模型，因为有一部分参数是 0，只需较少的内存来存储模型</p>

<p>在实践中发现，通过 L1 正则化让模型变得稀疏，带来的收益甚微，所以觉得至少在压缩模型的目标上他作用不大。</p>

<p>在训练神经网络时，L2 正则化使用得频繁很多</p>

<hr>

<p>lambda 被称为正则化参数，通常使用开发集或 hold-out 交叉验证，来配置参数。通过尝试一系列的值，找出最好的那个，即在“训练集上得到较好的结果”和“保持参数 w 的 L2 范数较小以避免过拟合”之间取舍</p>

<p>lambda 是你需要调优的另一个超参数</p>

<p>以上是在逻辑回归中实现 L2 正则化的方法</p>

</br>

## 神经网络

</br>

<p>在神经网络中，代价函数是所有参数的函数，w[1] b[1] 到 w[L] b[L]</p>

<p>至于正则化，再加上一项</p>

$$
J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l = 1}^{L}{\|w^{[l]}\|_F}^2
$$

$$
{\|w^{[l]}\|}^2 = \sum_{i = 1}^{n^{[l]}}\sum_{j = 1}^{n^{[l - 1]}}(w_{ij}^{[l]})^2
$$

<p>称为矩阵的弗罗贝尼乌斯范数</p>

</br>

## 实现梯度下降

</br>

<p>之前使用反向传播计算 dw，通过反向传播，能得到 J 关于 w 的偏导数</p>

$$
dW^{[l]} = (frombackprop)
$$

$$
W^{[l]} := W^{[l]} - \alpha dW^{[l]}
$$

<p>这是为目标函数添加正则化项之前的步骤，现在为目标函数添加了正则化项，就需要为 dw 加上一项</p>

$$
dW^{[l]} = (frombackprop) + \frac{\lambda}{m} W^{[l]}
$$

<p>可以证明，新的 dW[l] 仍然正确的定义了添加了额外的正则化项之后代价函数关于参数的导数</p>

<p>出于这个原因，L2 正则化有时也被称为权重衰减(Weight decay)</p>

<p>用新的 dw[l] 进行更新</p>

$$
W^{[l]} := (1 - \frac{\alpha \lambda}{m} )W^{[l]} - \alpha (frombackprop)
$$

<p>第一项(系数略小于 1)表示无论矩阵 W[l] 是多少，都会让他变得稍小一些，这就是 L2 范数正则化被称为权重衰减的原因</p>

</br>

# Why regularization reduces overfitting

![QQ_1745323720521](https://github.com/user-attachments/assets/f66e4202-400b-4c5a-a2d2-9b8a01601c03)

<p>可以认为第三个就是过拟合的神经网络</p>

<p></p>

$$
J(W^{[l]}, b^{[l]}) = \frac{1}{m} \sum_{i = 1}^{m} L(\hat{y}^{((i)}, y^{(i)}) 
$$























































































































































































































































































































































































































































































































































































