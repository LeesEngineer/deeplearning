# Regularization

</br>

<p>如果你怀疑你的神经网络在数据上发生了过拟合（存在高方差问题），首先尝试使用正则化，获取更多数据也是一个可靠的方法，但不是总能获取数据</p>

</br>

## 逻辑回归

</br>

<p>以逻辑回归为例子进行阐述，在逻辑回归中，会尝试最小化代价函数 J：</p>

$$
J(w, b) = \frac{1}{m} \sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) 
$$

<p>w 是 n_x 维的参数向量，b 是一个实数</p>

<p>要为逻辑回归正则化，需要加上 lambda，他称为正则化参数</p>

$$
J(w, b) = \frac{1}{m} \sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}{\|w\|_2}^2
$$

<p>其中：</p>

$$
{\|w\|_2}^2 = \sum_{j = 1}^{n_x}{w_j}^2 = w^T w 
$$

<p><b>这称为 L2 正则化</b>，其为 L2 范数</p>

<p>关于为何只对 w 进行正则化：事实上也可以加上 b 的范数，但通常会把他省略掉。</p>

<p>观察参数，w 往往是一个非常高维的参数矢量，尤其是发生在高方差问题的情况下，w 可能有非常多的参数。没能很好的拟合所有参数，而 b 只是单个数字，几乎所有参数都集中在 w 中，而不是 b。所以即使加上最后一项，也不会起到很大的作用（因为 b 只是大量参数中的一个）</p>

<p>L2 正则化是最常见的正则化方式</p>

<p>可能也听过 L1 正则化，即不使用 L2 范数，而是使用：</p>

$$
\frac{\lambda}{m}\sum_{j = 1}^{n_x}|w| = \frac{\lambda}{m}\|w\|_1
$$

<p>称为参数矢量 w 的 L1 范数（无论是 m 还是 2m 都只是一个缩放变量）</p>

<p>如果使用 L1 正则化，w 最后会变得稀疏，意味着 w 矢量中有很多 0。有人认为这有助于压缩模型，因为有一部分参数是 0，只需较少的内存来存储模型</p>

<p>在实践中发现，通过 L1 正则化让模型变得稀疏，带来的收益甚微，所以觉得至少在压缩模型的目标上他作用不大。</p>

<p>在训练神经网络时，L2 正则化使用得频繁很多</p>

<hr>

<p>lambda 被称为正则化参数，通常使用开发集或 hold-out 交叉验证，来配置参数。通过尝试一系列的值，找出最好的那个，即在“训练集上得到较好的结果”和“保持参数 w 的 L2 范数较小以避免过拟合”之间取舍</p>

<p>lambda 是你需要调优的另一个超参数</p>

<p>以上是在逻辑回归中实现 L2 正则化的方法</p>

</br>

## 神经网络

</br>

<p>在神经网络中，代价函数是所有参数的函数，w[1] b[1] 到 w[L] b[L]</p>

<p>至于正则化，再加上一项</p>

$$
J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l = 1}^{L}{\|w^{[l]}\|_F}^2
$$

$$
{\|w^{[l]}\|}^2 = \sum_{i = 1}^{n^{[l]}}\sum_{j = 1}^{n^{[l - 1]}}(w_{ij}^{[l]})^2
$$

<p>称为矩阵的弗罗贝尼乌斯范数</p>

</br>

## 实现梯度下降

</br>

<p>之前使用反向传播计算 dw，通过反向传播，能得到 J 关于 w 的偏导数</p>

$$
dW^{[l]} = (frombackprop)
$$

$$
W^{[l]} := W^{[l]} - \alpha dW^{[l]}
$$

<p>这是为目标函数添加正则化项之前的步骤，现在为目标函数添加了正则化项，就需要为 dw 加上一项</p>

$$
dW^{[l]} = (frombackprop) + \frac{\lambda}{m} W^{[l]}
$$

<p>可以证明，新的 dW[l] 仍然正确的定义了添加了额外的正则化项之后代价函数关于参数的导数</p>

<p>出于这个原因，L2 正则化有时也被称为权重衰减(Weight decay)</p>

<p>用新的 dw[l] 进行更新</p>

$$
W^{[l]} := (1 - \frac{\alpha \lambda}{m} )W^{[l]} - \alpha (frombackprop)
$$

<p>第一项(系数略小于 1)表示无论矩阵 W[l] 是多少，都会让他变得稍小一些，这就是 L2 范数正则化被称为权重衰减的原因</p>

</br>

# Why regularization reduces overfitting

![QQ_1745323720521](https://github.com/user-attachments/assets/f66e4202-400b-4c5a-a2d2-9b8a01601c03)

<p>可以认为第三个就是过拟合的神经网络</p>

<p>正则化所做的是添加一些额外的项目</p>

$$
J(W^{[l]}, b^{[l]}) = \frac{1}{m} \sum_{i = 1}^{m} L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l = 1}^{L}{\|w^{[l]}\|_F}^2
$$

<p>关于为何通过参数项就能减轻过拟合情况，一个直观理解就是，如果把正则参数 lambda 设置的很大，权重矩阵 W 就会被设置为非常接近 0 的值。因此这个直观理解就是把很多隐藏单元的权重设置的太接近 0 了而导致这些隐藏单元的影响被消除了</p>

![QQ_1745325149803](https://github.com/user-attachments/assets/de14b23a-b03b-48c3-9b31-014cccd7acaa)

<p>如果是这种情况，那么就会使这个大大简化的神经网络变成一个很小的神经网络，实际上，这种情况与逻辑回归单元很像，但很可能是网络的深度更大了，因此这就会使这个过拟合网络被带到更接近左图高偏差的状态。但是 lambda 存在一个中间值，能够得到一个更加接近中间图这个刚好的状态的</p>

<p>直观理解就是把 lambda 值设的足够高的话，他就会使 W 接近 0。但实际上不会发生这种情况，我们可以想象成通过把隐藏单元的值归零来削减了隐藏单元的影响，最终导致这个网络变成了一个更加简单的网络（越来越接近逻辑回归）</p>

<p>直觉上认为这些隐藏单元的影响被完全消除了，其实并不正确，实际上网络仍在使用所有的隐藏单元，但每个隐藏单元的影响变的非常小了，<b>最终得到的网络看起来就像一个不容易过拟合的小型的网络</b></p>

<hr>

<p>来看另一个例子来理解为什么正则化可以帮助防止过拟合</p>

<p>假设使用的激活函数是 tanh(z)</p>

<p>可以发现只要 z 的绝对值很小，比如 z 只涉及很小范围的参数</p>

![QQ_1745326279231](https://github.com/user-attachments/assets/32ba2f95-69a7-4409-83a9-f199f9eb3822)

<p>那么其实是在使用 tanh 函数的线性部分，当 z 的值能够取到很大或者很小的时候，激活函数才开始展现出他的非线性能力</p>

<p>因此直觉就是，如果 lambda 被设置的很大的话，那么激活函数的参数实际上就会变小（因为代价函数的参数会被限制导致不能过大），如果 w 变小，z 也会变小，特别是 z 的值相对都很小的时候，那么 tanh(z) 函数就会接近于线性函数。因此，每一层都几乎是线性的（就像线性回归一样）。</p>

<p>之前说过<b>如果每层都是线性的，那么整个网络就是线性网络。因此即使是一个很深的神经网络，如果使用线性激活函数，最终也只能计算线性的函数，因此就不能拟合那些很复杂的决策函数，也不能过度拟合那些数据集的非线形决策平面</b>，就像前面看到的过拟合高方差的情况</p>

![QQ_1745326835051](https://github.com/user-attachments/assets/e86228d7-0e0b-450f-aa46-04b6dbc5c931)

<p>总结：如果正则化(lambda)变得非常大，参数 W 就会很小，那么 Z 就会相对变小，即 z 只在小范围内取值（此时先忽略 b 的影响）。那么激活函数如果是 tanh 的话，这个激活函数就会呈现相对线性，那么整个神经网络就只能计算一些离线形函数很近的值，也就是相对比较简单的函数，而不能计算很复杂的非线性函数，因此就不太容易过拟合了（你会亲眼看到的）</p>

<p>以上就是 L2 正则化，也是我训练深度模型时最常用的正则化技巧，有时也会用到 drop-out 正则化</p>

</br>

# Dropout regularization

</br>

<p>另一种非常强大的正则化技术是“随机失活正则化”（丢弃法——dropout）</p>

<p>假设训练的神经网络存在过拟合，可以使用随机失活技术来处理。使用随机失活技术，要遍历这个网络的每一层，并且为丢弃(drop)网络中的某个节点设置一个概率值。即对于网络中的每一层，将对每一个节点做一次公平投币，使这个节点有 50% 的概率被保留，50% 的几率被丢弃，抛完硬币，决定清除哪些节点。然后清除哪些节点上所有正在进行的运算，所以最后得到的是一个小得多的被简化了很多的网络，然后再做反向传播训练</p>

![QQ_1745411427086](https://github.com/user-attachments/assets/e2bd355c-24e4-49c0-8a52-ac7485f016e7)

<p>看起来是一个 crazy 的技术，但确实是有效的</p>

</br>

## 实现

</br>

<p>有几种方法可以实现随机失活算法，最常用的一种--反向随机失活(inverted dropout)</p>

<p>在 l = 3 的层上演示如何实现单层的随机失活技术</p>

<p>设置一个矢量 d，d3 表示层 3 的失活向量</p>

`d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep-prob`

<p>判断 d3 中的某个元素小于某个值，命名为 keepprob，这是给定隐藏单元将被保留的概率值，如设置为 0.8，意味着隐藏单元有 0.2 的几率被丢弃。因此将生成一个随机的 0-1 矩阵</p>

`a3 = np.multiply(a3, d3)  # a3 *= d3`

<p>最后要放大 a3，除以 keep.prob 参数</p>

`a3 /= keep.prob`

<p>如果做矢量化的运算，a 的维数可能是 50(units)*m(x数量)，意味着将有平均十个 单元失活或者清零</p>

`z[4] = w[4] * a[3] + b[4]`

<p>a3 中 20% 的元素都被清零了。<b>为了不减少 z4 的期望值，需要除以 0.8，因为它能提供所需要的大约 20% 的校正值，这样 a3 的期望值就不会改变了</b></p>

<p>这就是所谓的反向随机失活技术(inverted dropout technique)，作用在于可以讲 keep.prob 设为任意值。<b>反向随机失活技术通过除以 keep.prob 确保 a3 的期望值不变</b></p>

<p>当在评估一个神经网络时，简化了神经网络的测试部分，因为减少了可能引入的缩放问题</p>

<p>早期没有除以 keep.prob 这一步，所以在测试过程中求平均值变得越来越复杂</p>

<p><b>不同的训练样例的训练，实际上对不同的隐藏单元实施了清零。实际上，如果用同一个训练集进行迭代，在不同的训练轮次中，也应该随机的将不同的隐藏单元清零。</b>因此这并不意味着同一个训练样例的训练应该保证一直丢弃相同的隐藏单元。d3 决定哪些将被清零(可以用不同的模式清零)</p>

</br>

## Making prediction at test time

</br>

```
No drop out 
z[1] = W[1] a[0] + b[1]
a[1] = g[1](z[1])
z[2] = W[2] a[1] + b[2]
a[2] = ...
```

<p>在测试阶段并没有使用随机失活算法，不用决定哪些隐藏单元要被消除，这是因为在测试阶段做预测的时候，并不想让输出也是随机的。如果在测试阶段也使用随机失活算法，只会为预测增加噪声</p>

<p>可以做的一件事情是，用不同的随机失活的神经网络进行多次预测并取平均值。但是该方法运算效率不高而且会得到几乎相同的预测结果，每次不同的预测过程将给出非常相似的结果</p>

<p>除以 keep.prob 作用是保证如果测试过程没有针对随机失活算法进行缩放(scaling)，那么激活函数的期望输出也不会改变。<b>所以不用在测试阶段过程中加入额外的缩放参数（这与训练过程不同）</b></p>

</br>

# Understanding dropout

</br>

<p>第一个 intuition：dropout 随机关闭神经网络中的神经元。就好像每次迭代都在使用一个较小的神经网络，使用较小的神经网络应该具有正则化效果</p>

<p>第二个 intuition（从单个 unit 的角度来看）：<b>Can't rely on any one feature,so have to spread out weights</b></p>

![QQ_1745477424344](https://github.com/user-attachments/assets/c84f08d0-a597-47e0-bc93-3e7f50244b25)

<p>例如这个单元，有四个输入，需要产生一些有意义的输出。现在，随着 dropout，输入可以被随机消除。现在这个 unit 不能依赖任何 feature，因为任何 feature 都可能随机消失（任何自己的输入都可能随机消失），特别是不愿把所有赌注都放在仅仅这个输入上。因此，这个 unit 将更有动力分散这种方式，让这个单元的四个输入中每一个都有权重。通过分散权重，往往会产生缩小权重平方范数的效果（这与在 L2 正则化中看到的情况类似,dropout 可以正式证明是 L2 正则化的一种自适应形式）</p>

![QQ_1745480449313](https://github.com/user-attachments/assets/c54263cd-e49b-42d3-8311-02de66f91e97)

```
w[1] is (7, 3)
w[2] is (7, 7)
w[3] is (3, 7)
```

<p><b>逐层调整 keep.prob 也是可行的</b>，W[2] 是最大的权重矩阵，因此为了防止该矩阵的过度拟合，也许对于这个层 keep.prob 可能相对较低，比如 0.5，W[1] 和 W[3] 可能是 0.7。对于后面的层，完全不需要担心过拟合，可以拥有 1 的 keep.prob，为 1 意味着保留了所有单位（没有在那一层中使用 dropout）</p>

<p>对于更担心过度拟合的层，是带有大量参数的层，you could say keep prop to be smaller to apply a more powerful form of dropout。p>

<p>还可以将 dropout 应用于输入层，这样就有机会只执行一个或多个输入特征(feature)，keep.prop 可以很高（0.9），通常不应用 dropout 进入输入层</p>

<p>总结：如果比其他层更担心某些层的拟合性，可以为这些层设置一个更小的 keep.prop。缺点是，为你提供了更多的超参数供你使用交叉验证进行搜索</p>

<p>CV 经常使用 dropout。只有计算机视觉，通常只是没有足够的数据，所以几乎总是过拟合</p>

<p>The thing to remember is that drop-out is a regularization technique,it helps prevent overfitting.And so unless my algorithm is overfitting,I wouldn't actually bother to use drop-out(要记住的一点是，Dropout 是一种正则化技术，它可以帮助防止过拟合。所以，除非我的算法真的出现了过拟合，否则我其实不会特意去用 Dropout。)</p>

</br>

# Other regularization methods

</br>

<p></p>



























































































































































































































































































































































































































































































































