# 训练集/开发集/测试集

</br>

<p>没什么好说的</p>

</br>

# 偏差与方差 Bias/Variance

</br>

<p>现在的趋势是讨论偏差，讨论方差，但减少讨论偏差方差权衡</p>

<p>给定一个数据集</p>
 
![QQ_1744803359059](https://github.com/user-attachments/assets/d55690bf-bf91-4ad4-80c1-8172c97387c0)

<p>对数据拟合一条直线，得到一个与之相适应的逻辑回归，与数据不太吻合<b>(underfitting)</b>，因此存在高偏差</p>

![QQ_1744803503772](https://github.com/user-attachments/assets/e4b3011c-23cb-431c-a750-387e0fdf6b9f)

<p>如果使用一个及其复杂的分类器(such as deep-neural-network or a new neural network with a lot hidden units)也许可以完美地容纳数据，但也看起来不太合适，因此这是一个具有高方差的分类器，这是对数据的 overfitting</p>

![QQ_1744803705264](https://github.com/user-attachments/assets/81466b5c-fad4-4bb7-b84c-3c7da99e5d93)

<p>使用复杂度中等的分类器，看起来更适合数据</p>

<p>在高维数据中研究别的不同的指标来尝试</p>

<p>通过查看训练集误差和开发集误差，能够对算法做出判断</p>

![QQ_1744804947539](https://github.com/user-attachments/assets/0cc1618c-a109-4fe7-a536-6232e9453bfc)

<p>通过了解 train set error 至少可以了解自己对训练数据的拟合程度，告诉你是否存在 bias</p>

<p>从 train set error 到 dev set error 可以了解方差问题的严重程度</p>

## High bias and high variance

![QQ_1744806167248](https://github.com/user-attachments/assets/96b7e7a3-3027-4858-995d-90fa4840b7f9)

<p>在蓝色线形分类器做一些奇怪的事情，那么它实际上也对部分数据进行了 overfit，紫色既有高偏差又有高方差</p>

<p>这里看起来有点太人为了，但实际上，输入维度高的时候，在某些区域会得到高偏差的东西，而在某些区域会得到高方差的东西。</p>

<p>因此，有可能在不那么人为的高维输入中获得这样的交叉文件</p>

</br>

# 机器学习的基本配方

</br>

## 1. 判断偏差

<p>当训练好了最初的神经网络时，首先会判断是否存在高偏差（实际上就是要看<b>训练集</b>的数据上的表现），如果存在高偏差(Underfitting)：</p>

- 挑选一个新的网络，比如带有更多隐藏层或更多隐藏单元的

- 延长训练时间

- 换用一些更高级的优化算法（后面提及，可能有效也可能无效，但神经网络的结构有许多种，所以可能会找到一种更加适合当前问题的结构）

<p>相比起来，使用 Bigger network 几乎总是有效。而延长训练时间，虽然并不永远有用，但也不会造成坏处</p>

## 2. 判断方差

<p>当把偏差减小到可以接受的范围之后，判断是否有高方差，要判断这一点，会看模型在<b>开发集</b>上的表现，看模型是否具有一般化的能力</p>

- 解决高方差的最好方法是取的更多数据，但有时无法获得更多数据

- 还可以尝试正则化，用它可以减少过拟合

- 还有一种需要亲自尝试的办法：如果能找到更合适的神经网络结构，有时也能够在缓解方差问题的同时也缓解偏差问题（倒是不太容易总结出完全系统性的规律）

<p>应先明确认识到是高偏差问题还是高方差问题，如果是高偏差问题就算取得再多的数据也无济于事（至少不是很有效率）</p>

<p>在早期机器学习时代，无法减小其一而不增大另一。</p>

<p>但在这个深度学习和大数据时代:</p>

- 只要能不断扩大所训练的网络的规模（解决高偏差），并且不断的获取更多数据（解决高方差），那么扩大网络几乎总是能够减小偏差而不增大方差

- 只要能用恰当的方式正则化，同时获得更多的数据，几乎总是能够减小方差而不增大偏差

<p>所以有了这两步(判断偏差，判断方差)以后，再加上能够选取不同的网络来训练，以及获取更多数据的能力。就有了能够只单独削减偏差或方差同时不会过多影响另一个指标的能力</p>

<b>很好解释了为何深度学习在监督学习中如此有用。以及为何在深度学习中，偏差与方差的权衡要不重要的多(这样就不需要小心地平衡两者)，而是因为有了更多选择，可以单独削减偏差或方差</b>

<p>事实上，当你有了一个良好的正则化的网络时，训练一个更大的网络几乎从来没有坏处。当训练的神经网络太大时主要的代价只是计算时间</p>

<p>正则化：用于减小方差的办法，在其中存在一些偏差与方差间的权衡，他可能使偏差增加一些（虽然当网络足够大时，增加的不会很多）</p>























































































































































































































































































































































































































































































































































































































































































