<p>本周要学习加快神经网络训练素的的优化算法，在大数据领域中，深度学习表现的并不算完美，能够训练集于大量数据的神经网络，而用大量数据就会很慢</p>
 
</br>

# Mini-batch gradient descent

</br>

<p>Vectorization allows you to efficiently compute on m examples，但是如果 m 非常大(5000000?)，速度依旧很慢。</p>

<p>对整个训练集运用梯度下降法，<b>必须先处理整个训练集，才能在梯度下降中往前走一步</b>，所以算法实际上是可以加快的，让梯度下降在处理完整个巨型的训练集之前就开始生效</p>

<p>首先将训练集拆分为更小的训练集，即小批量训练集(mini-batch)，比如说每一个微型训练集只有 1000 个训练样例(x1 to x1000, x1001 to x2000)，</p>

<p>新记号：X{1} 代表 x1 to x1000，类推。y 也做相应的拆分处理</p>

`Mini-batch t: X^{t}, Y^{t}`

<p>Batch: 同时处理整个训练集，名字由来就是同时处理整个训练集批次</p>

<p>mini-batch: 每次只处理一个小批量样例 X{t}, Y{t}</p>

</br>

## How it works

</br>

<p>在训练集上运行小批量梯度下降法的时候，每个子集都要运行一遍</p>

<p>for 循环里面要做的基本上就是用 (X{t}, Y{t}) 做一次梯度下降，用向量化的方法同时处理 1000 个样例</p>

```
for t = 1, ..., 5000
    ForwardProp on X{t}
        Z[1] = W[1] X{t} + b[1]
        A[1] = g[1](Z[1])
        ...
        A[L] = g[L](Z[L])
```





















































































































































































































