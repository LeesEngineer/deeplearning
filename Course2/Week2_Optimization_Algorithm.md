<p>本周要学习加快神经网络训练素的的优化算法，在大数据领域中，深度学习表现的并不算完美，能够训练集于大量数据的神经网络，而用大量数据就会很慢</p>
 
</br>

# Mini-batch gradient descent

</br>

<p>Vectorization allows you to efficiently compute on m examples，但是如果 m 非常大(5000000?)，速度依旧很慢。</p>

<p>对整个训练集运用梯度下降法，<b>必须先处理整个训练集，才能在梯度下降中往前走一步</b>，所以算法实际上是可以加快的，让梯度下降在处理完整个巨型的训练集之前就开始生效</p>

<p>首先将训练集拆分为更小的训练集，即小批量训练集(mini-batch)，比如说每一个微型训练集只有 1000 个训练样例(x1 to x1000, x1001 to x2000)，</p>

<p>新记号：X{1} 代表 x1 to x1000，类推。y 也做相应的拆分处理</p>

`Mini-batch t: X^{t}, Y^{t}`

<p>Batch: 同时处理整个训练集，名字由来就是同时处理整个训练集批次</p>

<p>mini-batch: 每次只处理一个小批量样例 X{t}, Y{t}，而不是一次处理完整个训练集 X Y  </p>

</br>

## How it works

</br>

<p>在训练集上运行小批量梯度下降法的时候，每个子集都要运行一遍</p>

<p>for 循环里面要做的基本上就是用 (X{t}, Y{t}) 做一次梯度下降，用向量化的方法同时处理 1000 个样例</p>

```
for t = 1, ..., 5000
    ForwardProp on X{t}
        Z[1] = W[1] X{t} + b[1]
        A[1] = g[1](Z[1])
        ...
        A[L] = g[L](Z[L])
        //上面这些都是矢量化方法（1000 examples）

    Compute cost
    Backprop to compute gradients
    W[l] := W[l] - alpha dW[l], b[l] := b[l] - alpha db[l]
```

$$
Cost: J^{\{t\}} = \sum_{i = 1}^{l = 1000}L(\hat{y}^{(i)}, y^{(i)}) / 1000 + \frac{\lambda}{2000} \sum_l \|W^{[l]}\|_F^2
$$

<p>这是小批量梯度下降算法处理训练集一轮的过程，也叫做训练集的一次遍历（epoch），遍历是指过一遍训练集。在批量梯度下降法中对训练集的一轮处理只能得到一步梯度逼近，而小批量梯度下降法中对训练集的一轮 epoch，可以得到 5000 步梯度逼近</p>

<p>当你有一个大型训练集时，小批量梯度下降法比梯度下降法快得多</p>

</br>

# Understanding mini-batch gradient descent

</br>

<p>在批量梯度下降算法中，每一次迭代将遍历整个训练集，用 J 来表示代价函数，那么他应该随着迭代单调递减。<b>如果某一次迭代他的值增加了，那么一定是哪里错了，比如学习率太大</b></p>

![QQ_1746254521244](https://github.com/user-attachments/assets/3b91b052-81c7-4496-8d2f-d72c9ea1e31f)

<p>而在小批量梯度下降中，同样画图就会发现并不是每一次迭代代价函数的值都会减小。从细节来看，每次迭代都是对 X{t} Y{t} 的处理，对通过他们计算出来的代价函数 J{t} 进行画图，就好像每次迭代都使用不同的训练集（也就是使用不同的 mini-batch），就会看到这样的图，他的趋势是向下的，但是也会有很多噪声</p>

![QQ_1746255297935](https://github.com/user-attachments/assets/00391dab-ed0a-4bb5-aeb0-ffda7636bf40)

<p>如果使用小批量梯度下降算法，经过几轮训练后，对 J{t} 作图很可能就像这样，并不是每次迭代都会下降，但是整体趋势必须是向下的</p>

<p>而他之所以有噪声，可能和计算代价函数时所用的那个批次 X{t}, Y{t} 有关，让代价函数的值或大或小。也有可能这个批次含有一些标签错误的数据，导致代价函数有一点高</p>

</br>

## Choosing your mini-batch size

</br>

<p>必须定义的一个参数是 mini-batch 的大小</p>

<p>一种极端情况 If mini-batch size = m，其实就是批量梯度下降。(X{1}, Y{1}) = (X, Y)</p>

<p>另一种极端情况则是把 mini-batch 设置为 1，会得到一种叫随机梯度下降的算法，每一条数据都是一个 mini-batch</p>

<p>看看两种方法在优化代价函数时有什么不同</p>

<p>批量梯度下降算法（假设从边缘开始）噪声相对小些，每一步相对较大，并且最终可以达到最小值</p>

![QQ_1746427972940](https://github.com/user-attachments/assets/c967f7d3-8b52-490a-9b45-710bb9f7e76f)

<hr>

<p>相对的，随机梯度下降算法（假设从这里开始），每一次迭代就在一个样本上做梯度下降，大多时候可以达到全局最小。但有时也可能因为某组数据不太好，把你指向一个错误的方向，因此随机梯度算法的噪声会非常大。一般来说会沿着一个正确的方向，而且<b>随机梯度下降算法最后也不会收敛到一个点，一般会在最低点附近摆动</b></p>

![QQ_1746430603323](https://github.com/user-attachments/assets/f3152a33-ebdc-49dd-b372-19064e143300)

<p><b>如果使用随机梯度下降算法，使用一个样本来更新梯度，这没有问题，而且可以通过选择比较小的学习率来减少噪声。但随机梯度下降有一个很大的缺点是失去了利用向量加速运算的机会</b></p>

<p>实际上 mini-batch 的大小会在这两个极端之间，既可以使用向量加速，也可以不用等整个训练集遍历完一遍才运行梯度下降</p>

![QQ_1746430664125](https://github.com/user-attachments/assets/8e580ae9-4be5-4e3b-a66b-40549eae8990)

<p>不能保证总能达到最小值，但相比随机梯度下降，他的噪声会更小，而且不会总在最小值附近摆动（如果有什么问题，可以缓慢的减小学习率，将会介绍学习率衰减）</p>

<p>准则：</p>

- 如果你的训练集较小（2000？），就使用批量梯度下降

- 较大的训练集，一般选择 64 - 512 作为 mini-batch 的大小（这是因为计算机内存的布局和访问模式，把 mini-batch 的大小设置为 2 的幂数会更快），1024 还是比较罕见的

- 确保所有的 X{t} Y{t} 是可以放进 CPU/GPU 内存的

</br>

# Exponentially weighted averages（指数加权平均值）

</br>

<p>展示几个优化算法，比梯度下降更快，为了理解他们，需要用到一种叫做指数加权平均的操作，在统计学上也被称为指数加权滑动平均，将使用这个概念来构建更复杂的优化算法</p>

![QQ_1746431767667](https://github.com/user-attachments/assets/09eeca13-385e-40fb-9c00-70ca286e7b2b)

<p>这些数据可能有噪声，如果想计算数据的趋势（即局部平均或滑动平均）</p>

```
V_0 = 0
V_1 = 0.9 V_0 + 0.1 theta_1
V_2 = 0.9 V_0 + 0.1 theta_2
...
V_t = 0.9 V_{t - 1} + 0.1 theta_t
```































































































































































































