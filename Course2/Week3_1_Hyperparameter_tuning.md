# Tuning process

</br>

<p>面对大量超参数：alpha, beta_1, beta_2, #layers, #hidden_units, learning_rate_decay, mini-batch_size</p>

<p>根据经验一部分超参数比其他的更加重要。</p>

- 第一档，alpha 是需要调优的超参数中最重要的一个。

- 第二档，单独使用 Gradient descent with momentum 时，beta 来说 0.9 是一个不错的值。还会调整 mini-batch——size，来保证最优化算法的运行效率。还有 #hidden_units 

- 第三档，网络层数有时对结果气到重要作用，学习率衰减有时也一样

- 第四档，当使用 Adam 优化算法时，几乎不调节 beta_1, beta_2, epsilon, 一般是 0.9, 0.999, 1e-8

<p>对于超参数的组合的选择，可以通过超参数值域的随机抽样，更有效的选择超参数空间</p>









































































































































































































































































































































































































































