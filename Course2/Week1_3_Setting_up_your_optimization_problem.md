</br>

# Normalizing inputs

</br>

<p>训练神经网络时，加快训练速度的方法之一是<b>对输入进行归一化</b></p>

<p>例举这个包含两个输入特征的训练集，如下为其散点图</p>

![QQ_1745561153396](https://github.com/user-attachments/assets/67ac959a-9643-453d-a396-e2712f9b2897)

<p>对输入进行归一化对应于两个步骤：</p>

<p>第一步是减去均值或归零</p>

<p>Subtrack mean(中心化):</p>

$$
\mu = \frac{1}{m} \sum_{i = 1}^{m}x^{(i)}
$$

$$
x := x - \mu
$$

![QQ_1745563241934](https://github.com/user-attachments/assets/d8543ae0-d3f5-4d06-b505-38f5c40953d3)

<hr>

<p>第二步是对方差进行归一化</p>

<p>此处的 feature x1 的方差比特征 x2 大得多。</p>

<p>Normalize variance(归一化):</p>

$$
\sigma^2 = \frac{1}{m} \sum_{i = 1}^{m} {x^{(i)}}^2
$$

$$
x = \frac{x}{\sigma}
$$

![QQ_1745564422812](https://github.com/user-attachments/assets/d3d8dc24-6d37-4c58-a9f7-bb1d83add9b5)

<p>得到这样的结果，现在 x1 和 x2 的方差都是 1</p>

<p>tip：如果使用它来缩放训练数据，则使用相同的 mu 和 sigma 来归一化测试集。特别是，你不想以不同的方式对训练集和测试集进行归一化。不管 mu 和 sigma 的值是什么，都要在 `x = \frac{x}{\sigma}` 和 `x := x - \mu` 这两个公式中使用它们，这样就可以用完全相同的方式缩放测试集，而不是分别估算测试集和训练集上的 mu 和 sigma square</p>

</br>

## 为什么要进行归一化

</br>

<p>事实证明，如果使用非标准化的输入特征，成本函数更有可能看起来如下这样，非常细长的成本函数</p>

![QQ_1745566222068](https://github.com/user-attachments/assets/eafc347e-fcb3-4860-b5ed-0aaa383bdb8f)

<p>如果特征比例截然不同，比如 x1 的范围是 1-1000，x2 的范围是 0-1，那么事实证明 w1(对应 x1) 和 w2(对应 x2) 的比率或值范围将采用截然不同的值</p>

<p>如果对 features 进行归一化，成本函数平均看起来会更加对称</p>

![QQ_1745566325251](https://github.com/user-attachments/assets/055ebda8-c385-44b1-b6f9-54c6c4c85df0)

<p>如果在 Unnormalized 成本函数上运行梯度不错，那么可能需要使用非常小的学习率，因为梯度可能需要很多步骤来回震荡才能达到最小值</p>

![QQ_1745566934752](https://github.com/user-attachments/assets/536532df-1d92-4492-b343-eb36e3a51787)

<p>当你的 feature 规模相似时，粗略的直觉是，成本函数会更加全面，更加容易优化</p>
 
<p>对于多特征来说，将他们全部设置为零均值，并设置方差为 1，就可以保证所有 feature 都处于相似的比例，帮助学习算法更快的运行</p>

<p>总结：如果 feature 来自截然不同的比例（0-1 和 1-1000），那么对 feature 进行归一化非常重要。如果 feature 以相似的比例出现，那么就不重要了</p>

</br>

# Vanishing/exploding gradients 

</br>

<p>当训练神经网络时会遇到一个问题----梯度的消失和爆炸，<b>尤其是训练层数非常多的神经网络时</b>。意思是当在训练一个深度神经网络时，损失函数的导数有时会变得非常大或非常小，这使得训练变得很困难</p>

<p>讨论梯度爆炸和消失的含义，以及如何谨慎的选择随机初始化的权重来显著减少这种问题的发生</p>

<p>假设正在训练一个层数很多的神经网络，会有参数 W[1],W[2],...W[l]，为了简单起见，设置 g(z) = z*（一个线性的激活函数），忽略 b</p>

<p>在这种情况下，Y = W[L] W[L - 1] ... W[2] W[1] x</p>



























































































































































































































































































































































































































































































